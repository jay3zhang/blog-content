# HTTP第二部分：HTTP结构

# 第五章-web服务器
* 建立连接
* 接收请求
* 处理请求
* 访问资源
* 构建响应
* 发送响应
* 日志记录

# 第六章

# 第七章

# 第八章

# 第九章-爬虫
* 相对链接标准化
	对于python来说，基于 base_url 用 jion函数
* 链接环路
避免重复爬取：记录爬取过的url，并且支持快速搜索。（海量数据管理）
	- 树和散列表
	- 分布式爬虫
	- 存在位图
	- 检查点
	python： 
	集合-set()
	MD5
	数据库
* 别名问题，规范化URL部分解决
	两个名字指向同一页面，会爬取两次，即别名问题。
	规范化：
	- 没有指定端口时，添加80端口。
	- 删除url中的#标签
	- 将所有转义字符 %xx 都转换成等价字符
* 文件系统环路-subdir
* 动态虚拟web空间-恶意web服务器生成假内容，日历等
* 避免循环和重复
	- 规范化URL
	- 广度优先爬行，减少爬取深度
	- 节流：
		- 限制一段时间内爬虫从一个站点获取页面的数量
		- 限制重复页面数
		- 限制对服务器的访问总数
	- 限制url长度，超出长度不爬取
	- 维护一个url黑名单
	- 模式检测-检测重复组件，解决文件系统环路和动态虚拟web问题
	- 内容指纹-基于内容相同去重（MD5），有时会忽略动态部分（如，日期、访问计数、嵌入的链接）

* 爬虫的HTTP首部
	- UA：爬虫名字
	- Accept：爬虫想要的文件类型
	- From：爬虫的用户的email
	- Host：访问虚拟主机时使用，即一台服务器上部署多个站点。

* 爬虫规范（web机器人操作员指南）-觉得这个特别好
	
# 第十章