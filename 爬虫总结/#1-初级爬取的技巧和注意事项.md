# 初级爬取的技巧和注意事项
1.1 检查sitemap文件
sitemap提供了所有网页的链接，但要谨慎处理，因为该文件经常存在缺失、过期或不完整的问题。

1.2 估算网站大小
用site关键字：搜索`site : example.webscraping.com`，会显示有多少条结果。

1.3 识别网站所用技术--builtwith模块
`builtwith.parse('url')` 返回所用技术

1.4 查找网站所有者--python-whois模块
`pip install python-whois`

`whois.whois('url')`

1.5 
* 当发生5xx错误时（即服务器发生问题时），重试下载，几次重试后放弃
* 用连续数值ID遍历网站时，某些记录可能已被删除，数据库ID之间不是连续的。此时访问到某个间隔点时，爬虫会出错退出。应该设置**连续**多次下载错误后退出。
* 遍历ID时，一些网站会检查页面别名是否满足预期，如果不是会返回404
* 链接爬虫：浏览器可以使用相对链接，urllib2无法获取上下文要用绝对链接。
* 用set()去重

1.6 高级功能
* 利用robots.txt，避开禁止爬取的url，可以将UA伪装成robots协议允许的UA。
* 支持代理（使用requests代理）
* 下载限速：用一个类记录每个url上次访问的时间，如果当前时间距离上次访问小于指定时，则执行睡眠，每次下载前调用该类。
简单一点的话，在每次下载前加一个time.sleep()
* 避免爬虫陷阱（比如日历）
设置爬取深度